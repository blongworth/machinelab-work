---
title: "Data exploration for LECS resampling"
format: html
editor: source
---

```{r}
library(tidyverse)
library(arrow)
library(dygraphs)
library(tictoc)
library(imputeTS)
library(rtide)

options(digits.secs = 6)

data_dir <- "data/processed/surface"
```

```{r}
ds <- open_dataset(file.path(data_dir, "lecs_adv_cal.parquet"))
names(ds)
nrow(ds)
```

Calculate duplicate fraction over a time period

No duplicates!

```{r}
dsf1 <- ds |> 
  filter(timestamp > as.POSIXct("2023-01-01 00:00:00"),
         timestamp < "2023-12-01 00:00:00") 

dss1 <- dsf1 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dsf2 <- ds |> 
  filter(timestamp > "2023-12-01 00:00:00",
         timestamp < "2024-06-01 00:00:00") 

dss2 <- dsf2 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dsf3 <- ds |> 
  filter(timestamp > "2024-06-01 00:00:00",
         timestamp < "2024-10-01 00:00:00") 

dss3 <- dsf3 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dss <- bind_rows(dss1, dss2, dss3)

ggplot(dss, aes(timestamp)) +
  geom_histogram(binwidth = 3600 * 24)
```

Early subset

```{r}
df <- ds |> 
  filter(timestamp > "2023-08-05 00:00:00",
         timestamp < "2023-08-06 00:00:00") |> 
  collect()
```

```{r}
df |> 
  select(timestamp, pressure) |> 
  dygraph() |> 
  dyRangeSelector()
```

```{r}
df |> 
  select(timestamp, pH_cal) |> 
  dygraph() |> 
  dyRangeSelector()
```

```{r}
df |> 
  filter(corr1 > 60,
         corr2 > 60,
         corr3 > 60) |> 
  select(timestamp, pH_cal) |> 
  dygraph() |> 
  dyRangeSelector()
```

```{r}
df |> 
  select(timestamp, amp1) |> 
  dygraph() |> 
  dyRangeSelector()
```

```{r}
df |> 
  ggplot(aes(u, corr1)) +
  geom_point()
```

Recent data

```{r}
df <- ds |> 
  filter(timestamp > "2024-08-05 00:00:00",
         timestamp < "2024-08-06 00:00:00") |> 
  collect()
```

```{r}
df |> 
  select(timestamp, pH_cal) |> 
  dygraph(group = "recent") |> 
  dyRangeSelector()
```


```{r}
df |> 
  select(timestamp, w) |> 
  dygraph(group = "recent") |> 
  dyRangeSelector()
```

```{r}
df |> 
  select(timestamp, amp1) |> 
  dygraph() |> 
  dyRangeSelector()
```

```{r}
df |> 
  ggplot(aes(u, corr1)) +
  geom_point()
```


```{r}
df |> 
  select(timestamp, pressure) |> 
  dygraph() |> 
  dyRangeSelector()
```

Minute averages

```{r}
ds_minute <- open_dataset(file.path(data_dir, "lecs_adv_minute.parquet"))
df_min <- ds_minute |> 
  collect()
```

```{r}
df_min |> 
  select(timestamp, pressure) %>% 
  dygraph(group = "minute") |> 
  dyRangeSelector()
```

```{r}
df_min |> 
  select(timestamp, u) %>% 
  dygraph(group = "minute") |> 
  dyRangeSelector()
```

```{r}
df_min |> 
  select(timestamp, pH_cal) %>% 
  dygraph(group = "minute") |> 
  dyRangeSelector()
```

hourly averages

```{r}
ds_hourly <- open_dataset(file.path(data_dir, "lecs_adv_hourly.parquet"))
ds_hourly |> 
  select(timestamp, pH_cal) %>% 
  collect() %>% 
  dygraph()
```

mean hourly correlation

```{r}
ds_hourly |> 
  select(timestamp, corr1, corr2, corr3) |> 
  collect() |> 
  rowwise() |> 
  mutate(mcorr = mean(c(corr1, corr2, corr3))) |> 
  select(timestamp, mcorr) |> 
  dygraph()
```

Compare pressure to predicted tide heights

```{r}
tp <- ds_hourly |> 
  mutate(Station = "Woods Hole, Buzzards Bay, Massachusetts",
         DateTime = timestamp) |> 
  collect() 
         
tp <- tide_height_data(tp)
```

```{r}
tp |> 
  select(timestamp, pressure, TideHeight) |> 
  dygraph() |> 
  dyRangeSelector()
```


4hz data

Check 4Hz resample. Should be missing sends.

```{r}
ds_4hz_all <- open_dataset(file.path(data_dir, "lecs_adv_4hz.parquet"))
```


Duplicates? No, resampling will average duplicates from upstream timeseries.

```{r}
ds_4hz_all |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()
```

```{r}
ds_4hz_all |> 
  select(timestamp, pH_cal) |>
  filter(timestamp > "2024-04-01 04:09:00") |> 
  arrange(timestamp) |>
  head(n = 1000) |>
  collect() |> 
  View()
```

Check imputed data. Should also have NA's for sends.

```{r}
ds_4hz_imp <- open_dataset(file.path(data_dir, "lecs_adv_4hz_imp_grp_10.parquet"))

test_imp <- ds_4hz_imp |> 
  select(timestamp, pH_cal, group, block) |>
  filter(timestamp > "2024-04-01 00:09:00") |> 
  arrange(timestamp) |>
  head(n = 1000) |>
  view()
```

```{r}
test_imp <- ds_4hz_imp |> 
  #select(timestamp, pH_cal, group, block) |>
  filter(floor_date(timestamp, "day") == as.Date("2024-04-01")) |> 
  arrange(timestamp) |> 
  collect()

ggplot(test_imp, aes(timestamp, pH_cal, color = block)) +
  geom_line()
```
```{r}
tic()
test_imp |> 
  select(timestamp, pH_cal) |> 
  dygraph()
toc()
```

```{r}
tic()
plotly::plot_ly(test_imp, x = ~timestamp, y = ~pH_cal, type = 'scatter', mode = 'lines')
toc()
```



Group sizes

```{r}
ds_4hz_imp_grp <- open_dataset(file.path(data_dir, "lecs_adv_4hz_imp_grp.parquet"))
```

```{r}
nrow(ds_4hz_imp_grp)

grp_size <- df_grp %>% 
  group_by(group) %>% 
  summarize(timestamp = min(timestamp), N = n(),
            dur = max(timestamp) - min(timestamp))
```

Number of > 15min groups per day

```{r}
grp_size |> 
  filter(N >= (4*60*10)) |> 
  mutate(timestamp = floor_date(timestamp, "day")) |> 
  group_by(timestamp) |> 
  summarize(N = n()) |> 
  collect() |> 
  ggplot(aes(timestamp, N)) + 
  geom_point()
  
```


```{r}
grp_day <- grp_size %>% 
  mutate(timestamp = floor_date(timestamp, "day")) |> 
  group_by(timestamp) |> 
  summarize(mean = mean(N),
            median = median(N), 
            min = min(N), 
            max = max(N),
  )
  
grp_day |> 
  collect() |> 
  pivot_longer(cols = c(min, median, max), values_to = "N", names_to = "sumtype") |> 
ggplot(aes(timestamp, N/4/60, color = sumtype)) + geom_point() +
  scale_y_log10() +
  labs(title = "Continuous group sizes by day",
       x = "",
       y = "Group length [min]")
```

Block sizes

```{r}
ds_4hz_imp_grp_10 <- open_dataset(file.path(data_dir, "lecs_adv_4hz_imp_grp_10.parquet"))
```

```{r}
ds_4hz_imp_grp_10 |> 
  select(timestamp, pH, pH_cal, group, block) |>
  filter(timestamp > "2024-04-01 04:09:00") |> 
  arrange(timestamp) |>
  head(n = 1000) |>
  collect() |> 
  View()
```

```{r}
grp_size <- ds_4hz_imp_grp_10 %>% 
  group_by(block) %>% 
  summarize(timestamp = min(timestamp), N = n())
```

Number of > 10min groups per day

```{r}
grp_size |> 
  filter(N >= (4*60*10)) |> 
  mutate(timestamp = floor_date(timestamp, "day")) |> 
  group_by(timestamp) |> 
  summarize(N = n()) |> 
  collect() |> 
  ggplot(aes(timestamp, N/(6*24))) + 
  geom_point() +
  ggtitle("ADV contiguous 10 min blocks") +
  ylab("Fraction of possible blocks per day") +
  ylim(0,1
       )
  
```

should not have groups longer than 10 min.

```{r}
grp_day <- grp_size %>% 
  mutate(timestamp = floor_date(timestamp, "day")) |> 
  group_by(timestamp) |> 
  summarize(mean = mean(N),
            median = median(N), 
            min = min(N), 
            max = max(N),
  )
  
grp_day |> 
  collect() |> 
  pivot_longer(cols = c(min, median, max), values_to = "N", names_to = "sumtype") |> 
ggplot(aes(timestamp, N/4/60, color = sumtype)) + geom_point() +
  #scale_y_log10() +
  labs(title = "Continuous group sizes by day",
       x = "",
       y = "Group length [min]") 
```

Daily distribution of groups

```{r}
grp_size <- collect(grp_size)

grp_size |> 
  filter(N >= 4 * 60 * 10) |> 
  ggplot(aes(hour(timestamp))) +
  geom_histogram(binwidth = 1, color = "lightblue")
```

Distribution of vertical velocity

```{r}

```

