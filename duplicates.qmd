---
title: "Duplicates"
format: html
editor: source
---

Investigating duplicate issue produced by processing code as of 2024-04-10.

Noted duplicate rows in processed adv data at 2024-01-01 05:00:07

```{r}
library(tidyverse)
library(mlabtools)
library(arrow)
library(tictoc)
library(Rcpp)
library(dygraphs)

options(digits.secs=3)
```

read processed data

read raw data

```{r}
df <- lecs_read_file("data/SD Card Data/LECS_surface_sd/lecs_surface_2024-01-23/2024_01_01_00_13_27.txt")
status <- lecs_status_data(df)
met <- lecs_met_data(df)
```

status jumps at beginning of new file due to buffered data (UDP? ADV serial?)

```{r}
status$timestamp[1:2]
```

note that times are actually local, but UTC is used to avoid EDT/EST issues.

```{r}
tsd <- status |> 
  mutate(tsd = c(NA, diff(timestamp)),
         adv_ts_diff = difftime(adv_timestamp, timestamp, units = "secs")) |> 
  filter(tsd != 1 | lead(tsd) != 1) |> 
  select(timestamp, adv_timestamp, adv_ts_diff, tsd)
```

Issue with remaining duplicate timestamps and most missing timestamps is 
that whenever S lines come in close to the teensy clock second boundary,
there's a chance that either two consecutive lines will come in
in the same second (duplicate timestamp), 
or two consecutive lines will come in slightly more than 1s apart 
(apparent missing timestamp), 
depending on when incoming serial bytes from the ADV are processed from the buffer.
I think we can fix this by using the ADV timestamp, which is always 1s apart. 
Need to think a little about keeping the ADV time corrected without introducing gaps.

look at when ADV timestamp diff changes over entire record

```{r}
status_all <- open_dataset("data/processed/status_all.parquet/") |> 
  collect()
```

find an example skippy period with otherwise good data

```{r}
stm <- status_all |> 
  #select(-adv_timestamp) |> 
  mutate(timestamp = floor_date(timestamp, "minutes")) |> 
  group_by(timestamp) |> 
  summarize(across(everything(), mean)) |> 
  collect()

stm |> 
  select(orig_timestamp, timestamp, adv_timestamp, adv_timestamp_cor) |> 
  View()
```

```{r}
dygraph(select(status_all, timestamp, bat)) |> 
  dyRangeSelector()
```

```{r}
ts <- status_all |> 
  select(timestamp, adv_timestamp) |> 
  filter(timestamp > "2023-10-29", 
         timestamp < "2023-10-30") |> 
  arrange(timestamp)
```

```{r}
status_all |> 
  group_by(timestamp) |> 
  filter(n() > 1)
```

```{r}
adv_all <- open_dataset("data/processed/adv_all_filt_cal.parquet/") 

nrow(adv_all)

adv_d <- adv_all |> 
  count(timestamp) |> 
  filter(n > 1) |> 
  collect()
nrow(adv_d)
```

```{r}
status_all |> 
  filter(heading != 0) |> 
  View()
```

toy example for loop correction for duplicates

```{r}
x <- c(1,2,2,3,5,5,7,10)

for (i in 2:length(x)) {
  if (x[i] == x[i-1]) {
    x[i] = x[i] + 1
  }  
}

x
```

for loop correction of 1 day of data


```{r}
tsv <- ts[["timestamp"]]

tic()
for (i in 2:length(tsv)) {
  if (tsv[i] == tsv[i-1]) {
    tsv[i] = tsv[i] + 1
  }
}
toc()
```
too slow! would take >1h to correct all status data

running the loop in C++ is much faster!

```{r}
x <- ts[["timestamp"]]
y <- ts[["adv_timestamp"]]

Rcpp::cppFunction('
IntegerVector fix_status_timestamps(IntegerVector timestamp, IntegerVector adv_timestamp) {
  for (int i = 0; i < timestamp.size(); i++) {
    if (timestamp[i] == timestamp[i - 1]) {
      timestamp[i] = timestamp[i] + 1;
    }
    if (adv_timestamp[i] == adv_timestamp[i - i] + 1 && timestamp[i] == timestamp[i - 1] + 2) {
      timestamp[i] = timestamp[i] - 1;
    }
  }
  return timestamp;
}
')
tic()
xc <- fix_status_timestamps(x, y)
toc()
```

check results

```{r}
xcdf <- tibble(xc, ts)

xcdf <- xcdf |> 
  mutate(diff = c(NA, diff(xc)),
         org_diff = c(NA, diff(timestamp)),
         adv_diff = c(NA, diff(adv_timestamp)),
         adv_teensy_diff = difftime(adv_timestamp, timestamp, units = "secs"))
```

No duplicates!

```{r}
filter(xcdf, diff == 0)
```

still some gaps of 2 or more

```{r}
xcdf |> filter(diff != 1) |> nrow()
```

better than the original by a factor of 200

```{r}
xcdf |> filter(org_diff != 1) |> nrow()
```

```{r}
xcdf |> filter(adv_diff != 1) |> nrow()
```

```{r}
xcdf |> filter(adv_diff == 2) |> nrow()
```
what's happening there?

```{r}
xcdf <- xcdf |> 
  mutate(miss = diff > 1 | lag(diff) > 1 | lead(diff) > 1 | lead(diff, 2) > 1)

xcdf |> filter(miss)
```

another try using offset from ADV time

```{r}
correct_status_timestamp_adv <- function(timestamp, adv_timestamp) {
  offset <- difftime(timestamp[1], adv_timestamp[1], units = "secs")
  adv_timestamp + offset
}

tsc <- ts |> 
  mutate(ts_cor = correct_status_timestamp_adv(timestamp, adv_timestamp))
```

This works great for this test set and all data from the 2024-01-23 download,
but causes garbage data with earlier datasets.

these methods will have issues with triplicate or more timestamps.
looks like there are a few of those in the data

```{r}
vec <- as.integer(tsda[["timestamp"]])

# Find runs of consecutive values
run_lengths <- rle(vec)

# Find values with three or more consecutive duplicates
three_or_more <- run_lengths$values[run_lengths$lengths >= 3]

hist(three_or_more)

three_or_moret <- as.POSIXct(three_or_more)
three_or_moret
```