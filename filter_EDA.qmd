---
title: "LECS EDA for filtering data"
format: html
editor: source
---

Find bad sections in data and limits for value-based filtering.

```{r}
library(arrow)
library(tidyverse)
library(dygraphs)
library(duckdb)

options(digits.secs = 6)

data_dir <- "data/processed/surface"
con <- dbConnect(duckdb(), dbdir = "my-db.duckdb", read_only = FALSE)
```

Open dataset

```{r}
ds <- open_dataset(file.path(data_dir, "lecs_adv_data.parquet"))
```


```{r}
names(ds)
```


```{r}
nrow(ds)
```


```{r}
summarize(ds, start = min(timestamp), end = max(timestamp)) %>% collect()
```


```{r}
ds
```

Duplicates?

```{r}
ds |> 
  filter(timestamp > "2023-08-05 00:00:00",
         timestamp < "2023-08-06 00:00:00") |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()
```

```{r}
dsf1 <- ds |> 
  filter(timestamp > "2023-01-01 00:00:00",
         timestamp < "2023-12-01 00:00:00") 

dss1 <- dsf1 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dsf2 <- ds |> 
  filter(timestamp > "2023-12-01 00:00:00",
         timestamp < "2024-06-01 00:00:00") 

dss2 <- dsf2 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dsf3 <- ds |> 
  filter(timestamp > "2024-06-01 00:00:00",
         timestamp < "2024-10-01 00:00:00") 

dss3 <- dsf3 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dss <- bind_rows(dss1, dss2, dss3)

ggplot(dss, aes(timestamp)) +
  geom_histogram(binwidth = 3600 * 24)
```

Are these cross-file duplicates due to timestamps out of whack?
If so, why so few?

```{r}
vs <- ds |>
  to_duckdb(con = con) |> 
  slice_sample(n = 100000) |>
  #filter(!is.na(u), !is.na(v), !is.na(w)) |> 
  collect()
```

```{r}
summarize(vs, start = min(timestamp), end = max(timestamp)) %>% collect()
```

Arrow does not slice datasets randomly

```{r}
sample_test <- function(i) {
  ds |>
    select(timestamp) |> 
    slice_sample(n = 10000) |>
    summarize(start = min(timestamp), end = max(timestamp)) |> 
    collect() |> 
    mutate(i = i)
}

map(1:5, sample_test) |> 
  list_rbind()
```

duckdb does!

```{r}
dsd <- to_duckdb(ds, con = con)
sample_test <- function(i) {
  dsd |>
    select(timestamp) |> 
    slice_sample(n = 10000) |>
    summarize(start = min(timestamp), end = max(timestamp)) |> 
    collect() |> 
    mutate(i = i)
}

map(1:5, sample_test) |> 
  list_rbind()
```

```{r}
vs |> 
  select(timestamp, u, v, w) |> 
  dygraph() |> 
  dyRangeSelector(
  )
```

```{r}
vs |>
pivot_longer(cols = c(u, v, w), names_to = "axis", values_to = "velocity") |>
ggplot(aes(velocity)) +
geom_density() +
facet_grid(rows = "axis", scales = "free_y")
```


```{r}
vs |>
pivot_longer(cols = c(u, v, w), names_to = "axis", values_to = "velocity") |>
ggplot(aes(velocity)) +
geom_density() +
scale_y_log10() +
  #xlim(-1,1) +
facet_grid(rows = "axis", scales = "free_y")

```

```{r}
summarize(vs, across(everything(), list(mean = \(x) mean(x, na.rm = TRUE), sd = \(x) sd(x, na.rm=TRUE))))
```

1.5 * IQR range

```{r}
qrange <- function(x, factor = 1.5) {
  x <- na.exclude(x)
  q1 <- quantile(x, .25)
  q3 <- quantile(x, .75)
  qr <- q3 - q1
  list(min = q1 - (qr * factor), 
       max = q3 + (qr * factor))
}

qr <- vs |> 
  select(u, v, w) |> 
  map(qrange, factor = 3)
```


fall spike data

```{r}
df <- ds |> 
  filter(timestamp > "2023-12-02 20:15:00",
         timestamp < "2023-12-03 06:00:00") |> 
  collect()

# dfsnip <- ds |> 
#   filter(timestamp > "2023-11-28 20:00:00",
#          timestamp < "2023-11-28 20:01:00") |> 
#   collect()

#write_csv(dfsnip, "bad_data_16Hz.csv")

df_long <- df |> pivot_longer(cols = c(u, v, w), names_to = "axis", values_to = "velocity")

df_long |> 
  ggplot(aes(timestamp, velocity)) +
  geom_line() +
  #xlim()
  facet_grid(rows = "axis", scales = "free")
```

```{r}
df |> 
  select(timestamp, corr1, corr2, corr3) |> 
  dygraph() |> 
  dyRangeSelector(
  )
```

```{r}
df |> 
  select(timestamp, u, v, w) |> 
  dygraph() |> 
  dyRangeSelector(
  )
```
# Minute averages

Open minute averages to get proper ranges. Need to create hour or minute average dataset to do this

```{r}
df_min <- ds |> 
  mutate(timestamp = floor_date(timestamp, "minute")) |> 
  group_by(timestamp) |> 
  summarise(across(everything(), 
                   ~ mean(.x, na.rm = TRUE))) |> 
  ungroup() |> 
  collect()
```

```{r}
df_min |> 
  ggplot(aes(timestamp, u)) +
  geom_line()
```

```{r}
df_min |> 
  ggplot(aes(timestamp, ox_umol_l)) +
  geom_line()
```

```{r}
df_min |> 
  select(timestamp, pH) |> 
  dygraph()
```

```{r}
df_min |> 
  select(timestamp, ox_umol_l) |> 
  dygraph()
```

```{r}
df_min |> 
  ggplot(aes(timestamp, pH)) +
  geom_line()
```

```{r}
ds |> 
  glimpse()
```

```{r}
df_min |> 
  select(timestamp, u) |> 
  dygraph() |> 
  dyRangeSelector(
  )
```



# Check filtered data

```{r}
ds_filt <- open_dataset(file.path(data_dir, "lecs_adv_filt.parquet"))
```


```{r}
nrow(ds_filt)
```


```{r}
summarize(ds_filt, start = min(timestamp), end = max(timestamp)) %>% collect()
```

Not sure what causes these more recent duplicates.
Calculate duplicate fraction over a time period


```{r}
dsf1 <- ds_filt |> 
  filter(timestamp > "2023-01-01 00:00:00",
         timestamp < "2023-12-01 00:00:00") 

dss1 <- dsf1 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dsf2 <- ds_filt |> 
  filter(timestamp > "2023-12-01 00:00:00",
         timestamp < "2024-06-01 00:00:00") 

dss2 <- dsf2 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dsf3 <- ds_filt |> 
  filter(timestamp > "2024-06-01 00:00:00",
         timestamp < "2024-10-01 00:00:00") 

dss3 <- dsf3 |> 
  group_by(timestamp) |> 
  summarize(n = n()) |> 
  filter(n > 1) |> 
  collect()

dss <- bind_rows(dss1, dss2, dss3)

ggplot(dss, aes(timestamp)) +
  geom_histogram(binwidth = 3600 * 24)
```
